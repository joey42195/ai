{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joey42195/ai/blob/main/nb/Deepseek_OCR_(3B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hu95jDOut4n"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kp6H_Y6ut4t"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJJ7rIHaut4u"
      },
      "source": [
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9gH5UKvut47"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-5usUE5Aut48"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install jiwer\n",
        "!pip install einops addict easydict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EqIhdcaut5C"
      },
      "source": [
        "Let's prepare the OCR model to our local first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "dcf4425815be4f1e828e1613fac192c6",
            "1fb70cc6203c4f0ba7b3eb8b5108ede5",
            "211fc78c7964460d8da91cdc05a733f2",
            "1891a0e06f204824b076a6a59c0a7417",
            "44853c7f0a9f49a3bcfd37c812ced9bb",
            "e7b6271ce623488ab47bf523152cc550",
            "8fc37e4b3c13469491264c50d5ec99ab",
            "a0150cd6a2914e1da57832da57ff7674",
            "42c3c2ca20504432995e86ea6ce95c19",
            "4c77831542aa46519cd3aaa7ebd48ae4",
            "1edb8cd1c45a48e9baca24beb47e4c16"
          ]
        },
        "id": "C6XSYMraut5D",
        "outputId": "be737ea0-b3e5-4ade-b2a5-31daf159c4f7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcf4425815be4f1e828e1613fac192c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/deepseek_ocr'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "a11389cb-4494-47f8-db9d-93ba29450977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.11.3: Fast Deepseekocr patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at ./deepseek_ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "import os\n",
        "os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", # Qwen 3 vision support\n",
        "    \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"./deepseek_ocr\",\n",
        "    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    auto_model = AutoModel,\n",
        "    trust_remote_code=True,\n",
        "    unsloth_force_compile=True,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfZVh2ByxFNh"
      },
      "source": [
        "### Let's Evaluate Deepseek-OCR Baseline Performance on Persian Transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"leo009/test-ocr\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YZHjLlxrI_TH"
      },
      "outputs": [],
      "source": [
        "dataset[0]['image'].save(\"your_image.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "fpS8-2lHCEOJ",
        "outputId": "464bad58-662c-4cca-f629-cf1cc971ec12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=379x32>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "dataset[0]['image']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eHGXRjjM5G5t",
        "outputId": "ed62b6a9-bac4-4d85-cdcc-13c6c22541f0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'磁场。不加外磁场时，原子在两个'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6LYHC3F0Ihf"
      },
      "source": [
        "<h3>Baseline Model Performance: 23% Character Error Rate (CER) for this sample !</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzckMII_02s_"
      },
      "source": [
        "# Let's finetune Deepseek-OCR !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "b0e8ef6d-fea9-411f-9596-b730114df7d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We'll be using a dataset for Persian OCR. The goal is to convert these images into a computer readable form - ie text. This can be very useful for digitizing Persian text.\n",
        "\n",
        "You can access the dataset [here](https://huggingface.co/datasets/hezarai/parsynth-ocr-200k).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
        "\n",
        "```python\n",
        "[\n",
        "{ \"role\": \"<|User|>\",\n",
        "  \"content\": \"\",\n",
        "  \"images\": []\n",
        "},\n",
        "{ \"role\": \"<|Assistant|>\",\n",
        "  \"content\": \"\"\n",
        "},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oPXzJZzHEgXe"
      },
      "outputs": [],
      "source": [
        "instruction = \"<image>\\\\nFree OCR. \"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    conversation = [\n",
        "        { \"role\": \"<|User|>\", \"content\": instruction, \"images\": [sample['image']] },\n",
        "        { \"role\": \"<|Assistant|>\", \"content\": sample[\"text\"] }\n",
        "    ]\n",
        "    return { \"messages\" : conversation }\n",
        "\n",
        "dataset = load_dataset(\"leo009/test-ocr\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-9u-OD6_gE"
      },
      "source": [
        "Let's convert the dataset into the \"correct\" format for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NLHM0PGn30x5"
      },
      "outputs": [],
      "source": [
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "We look at how the conversations are structured for the first example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFzmplrEy9I",
        "outputId": "d3c6d4b8-32d6-49ce-d4c4-22832b564325"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': '<|User|>',\n",
              "   'content': '<image>\\\\nFree OCR. ',\n",
              "   'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=379x32>]},\n",
              "  {'role': '<|Assistant|>', 'content': '磁场。不加外磁场时，原子在两个'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "converted_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "E2WR-p20LcG_"
      },
      "outputs": [],
      "source": [
        "# @title Create datacollator\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from PIL import Image, ImageOps\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import io\n",
        "\n",
        "from deepseek_ocr.modeling_deepseekocr import (\n",
        "    format_messages,\n",
        "    text_encode,\n",
        "    BasicImageTransform,\n",
        "    dynamic_preprocess,\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class DeepSeekOCRDataCollator:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        tokenizer: Tokenizer\n",
        "        model: Model\n",
        "        image_size: Size for image patches (default: 640)\n",
        "        base_size: Size for global view (default: 1024)\n",
        "        crop_mode: Whether to use dynamic cropping for large images\n",
        "        train_on_responses_only: If True, only train on assistant responses (mask user prompts)\n",
        "    \"\"\"\n",
        "    tokenizer: Any\n",
        "    model: Any\n",
        "    image_size: int = 640\n",
        "    base_size: int = 1024\n",
        "    crop_mode: bool = True\n",
        "    image_token_id: int = 128815\n",
        "    train_on_responses_only: bool = True\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        model,\n",
        "        image_size: int = 640,\n",
        "        base_size: int = 1024,\n",
        "        crop_mode: bool = True,\n",
        "        train_on_responses_only: bool = True,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.image_size = image_size\n",
        "        self.base_size = base_size\n",
        "        self.crop_mode = crop_mode\n",
        "        self.image_token_id = 128815\n",
        "        self.dtype = model.dtype  # Get dtype from model\n",
        "        self.train_on_responses_only = train_on_responses_only\n",
        "\n",
        "        self.image_transform = BasicImageTransform(\n",
        "            mean=(0.5, 0.5, 0.5),\n",
        "            std=(0.5, 0.5, 0.5),\n",
        "            normalize=True\n",
        "        )\n",
        "        self.patch_size = 16\n",
        "        self.downsample_ratio = 4\n",
        "\n",
        "        # Get BOS token ID from tokenizer\n",
        "        if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:\n",
        "            self.bos_id = tokenizer.bos_token_id\n",
        "        else:\n",
        "            self.bos_id = 0\n",
        "            print(f\"Warning: tokenizer has no bos_token_id, using default: {self.bos_id}\")\n",
        "\n",
        "    def deserialize_image(self, image_data) -> Image.Image:\n",
        "        \"\"\"Convert image data (bytes dict or PIL Image) to PIL Image in RGB mode\"\"\"\n",
        "        if isinstance(image_data, Image.Image):\n",
        "            return image_data.convert(\"RGB\")\n",
        "        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
        "            image_bytes = image_data['bytes']\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            return image.convert(\"RGB\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n",
        "\n",
        "    def calculate_image_token_count(self, image: Image.Image, crop_ratio: Tuple[int, int]) -> int:\n",
        "        \"\"\"Calculate the number of tokens this image will generate\"\"\"\n",
        "        num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "        num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "        width_crop_num, height_crop_num = crop_ratio\n",
        "\n",
        "        if self.crop_mode:\n",
        "            img_tokens = num_queries_base * num_queries_base + 1\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                img_tokens += (num_queries * width_crop_num + 1) * (num_queries * height_crop_num)\n",
        "        else:\n",
        "            img_tokens = num_queries * num_queries + 1\n",
        "\n",
        "        return img_tokens\n",
        "\n",
        "    def process_image(self, image: Image.Image) -> Tuple[List, List, List, List, Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Process a single image based on crop_mode and size thresholds\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio)\n",
        "        \"\"\"\n",
        "        images_list = []\n",
        "        images_crop_list = []\n",
        "        images_spatial_crop = []\n",
        "\n",
        "        if self.crop_mode:\n",
        "            # Determine crop ratio based on image size\n",
        "            if image.size[0] <= 640 and image.size[1] <= 640:\n",
        "                crop_ratio = (1, 1)\n",
        "                images_crop_raw = []\n",
        "            else:\n",
        "                images_crop_raw, crop_ratio = dynamic_preprocess(\n",
        "                    image, min_num=2, max_num=9,\n",
        "                    image_size=self.image_size, use_thumbnail=False\n",
        "                )\n",
        "\n",
        "            # Process global view with padding\n",
        "            global_view = ImageOps.pad(\n",
        "                image, (self.base_size, self.base_size),\n",
        "                color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "            )\n",
        "            images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            width_crop_num, height_crop_num = crop_ratio\n",
        "            images_spatial_crop.append([width_crop_num, height_crop_num])\n",
        "\n",
        "            # Process local views (crops) if applicable\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                for crop_img in images_crop_raw:\n",
        "                    images_crop_list.append(\n",
        "                        self.image_transform(crop_img).to(self.dtype)\n",
        "                    )\n",
        "\n",
        "            # Calculate image tokens\n",
        "            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n",
        "                    num_queries * height_crop_num)\n",
        "\n",
        "        else:  # crop_mode = False\n",
        "            crop_ratio = (1, 1)\n",
        "            images_spatial_crop.append([1, 1])\n",
        "\n",
        "            # For smaller base sizes, resize; for larger, pad\n",
        "            if self.base_size <= 640:\n",
        "                resized_image = image.resize((self.base_size, self.base_size), Image.LANCZOS)\n",
        "                images_list.append(self.image_transform(resized_image).to(self.dtype))\n",
        "            else:\n",
        "                global_view = ImageOps.pad(\n",
        "                    image, (self.base_size, self.base_size),\n",
        "                    color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "                )\n",
        "                images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "        return images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio\n",
        "\n",
        "    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Process a single conversation into model inputs.\n",
        "            \"\"\"\n",
        "\n",
        "            # --- 1. Setup ---\n",
        "            images = []\n",
        "            for message in messages:\n",
        "                if \"images\" in message and message[\"images\"]:\n",
        "                    for img_data in message[\"images\"]:\n",
        "                        if img_data is not None:\n",
        "                            pil_image = self.deserialize_image(img_data)\n",
        "                            images.append(pil_image)\n",
        "\n",
        "            if not images:\n",
        "                raise ValueError(\"No images found in sample. Please ensure all samples contain images.\")\n",
        "\n",
        "            tokenized_str = []\n",
        "            images_seq_mask = []\n",
        "            images_list, images_crop_list, images_spatial_crop = [], [], []\n",
        "\n",
        "            prompt_token_count = -1 # Index to start training\n",
        "            assistant_started = False\n",
        "            image_idx = 0\n",
        "\n",
        "            # Add BOS token at the very beginning\n",
        "            tokenized_str.append(self.bos_id)\n",
        "            images_seq_mask.append(False)\n",
        "\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "\n",
        "                # Check if this is the assistant's turn\n",
        "                if role == \"<|Assistant|>\":\n",
        "                    if not assistant_started:\n",
        "                        # This is the split point. All tokens added *so far*\n",
        "                        # are part of the prompt.\n",
        "                        prompt_token_count = len(tokenized_str)\n",
        "                        assistant_started = True\n",
        "\n",
        "                    # Append the EOS token string to the *end* of assistant content\n",
        "                    content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n",
        "\n",
        "                # Split this message's content by the image token\n",
        "                text_splits = content.split('<image>')\n",
        "\n",
        "                for i, text_sep in enumerate(text_splits):\n",
        "                    # Tokenize the text part\n",
        "                    tokenized_sep = text_encode(self.tokenizer, text_sep, bos=False, eos=False)\n",
        "                    tokenized_str.extend(tokenized_sep)\n",
        "                    images_seq_mask.extend([False] * len(tokenized_sep))\n",
        "\n",
        "                    # If this text is followed by an <image> tag\n",
        "                    if i < len(text_splits) - 1:\n",
        "                        if image_idx >= len(images):\n",
        "                            raise ValueError(\n",
        "                                f\"Data mismatch: Found '<image>' token but no corresponding image.\"\n",
        "                            )\n",
        "\n",
        "                        # Process the image\n",
        "                        image = images[image_idx]\n",
        "                        img_list, crop_list, spatial_crop, tok_img, _ = self.process_image(image)\n",
        "\n",
        "                        images_list.extend(img_list)\n",
        "                        images_crop_list.extend(crop_list)\n",
        "                        images_spatial_crop.extend(spatial_crop)\n",
        "\n",
        "                        # Add image placeholder tokens\n",
        "                        tokenized_str.extend(tok_img)\n",
        "                        images_seq_mask.extend([True] * len(tok_img))\n",
        "\n",
        "                        image_idx += 1 # Move to the next image\n",
        "\n",
        "            # --- 3. Validation and Final Prep ---\n",
        "            if image_idx != len(images):\n",
        "                raise ValueError(\n",
        "                    f\"Data mismatch: Found {len(images)} images but only {image_idx} '<image>' tokens were used.\"\n",
        "                )\n",
        "\n",
        "            # If we never found an assistant message, we're in a weird state\n",
        "            # (e.g., user-only prompt). We mask everything.\n",
        "            if not assistant_started:\n",
        "                print(\"Warning: No assistant message found in sample. Masking all tokens.\")\n",
        "                prompt_token_count = len(tokenized_str)\n",
        "\n",
        "            # Prepare image tensors\n",
        "            images_ori = torch.stack(images_list, dim=0)\n",
        "            images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n",
        "\n",
        "            if images_crop_list:\n",
        "                images_crop = torch.stack(images_crop_list, dim=0)\n",
        "            else:\n",
        "                images_crop = torch.zeros((1, 3, self.base_size, self.base_size), dtype=self.dtype)\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n",
        "                \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n",
        "                \"images_ori\": images_ori,\n",
        "                \"images_crop\": images_crop,\n",
        "                \"images_spatial_crop\": images_spatial_crop_tensor,\n",
        "                \"prompt_token_count\": prompt_token_count, # This is now accurate\n",
        "            }\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate batch of samples\"\"\"\n",
        "        batch_data = []\n",
        "\n",
        "        # Process each sample\n",
        "        for feature in features:\n",
        "            try:\n",
        "                processed = self.process_single_sample(feature['messages'])\n",
        "                batch_data.append(processed)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not batch_data:\n",
        "            raise ValueError(\"No valid samples in batch\")\n",
        "\n",
        "        # Extract lists\n",
        "        input_ids_list = [item['input_ids'] for item in batch_data]\n",
        "        images_seq_mask_list = [item['images_seq_mask'] for item in batch_data]\n",
        "        prompt_token_counts = [item['prompt_token_count'] for item in batch_data]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first=True, padding_value=False)\n",
        "\n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Mask padding tokens\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # Mask image tokens (model shouldn't predict these)\n",
        "        labels[images_seq_mask] = -100\n",
        "\n",
        "        # Mask user prompt tokens when train_on_responses_only=True (only train on assistant responses)\n",
        "        if self.train_on_responses_only:\n",
        "            for idx, prompt_count in enumerate(prompt_token_counts):\n",
        "                if prompt_count > 0:\n",
        "                    labels[idx, :prompt_count] = -100\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "        # Prepare images batch (list of tuples)\n",
        "        images_batch = []\n",
        "        for item in batch_data:\n",
        "            images_batch.append((item['images_crop'], item['images_ori']))\n",
        "\n",
        "        # Stack spatial crop info\n",
        "        images_spatial_crop = torch.cat([item['images_spatial_crop'] for item in batch_data], dim=0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"images\": images_batch,\n",
        "            \"images_seq_mask\": images_seq_mask,\n",
        "            \"images_spatial_crop\": images_spatial_crop,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
        "\n",
        "We use our new `DeepSeekOCRDataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from unsloth import is_bf16_supported\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "data_collator = DeepSeekOCRDataCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    train_on_responses_only=True,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = converted_dataset,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 10,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        fp16 = not is_bf16_supported(),\n",
        "        bf16 = is_bf16_supported(),\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        dataloader_num_workers=2,\n",
        "        remove_unused_columns = False,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Upload image\n",
        "print(\"請上傳發票圖片:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    image_filename = list(uploaded.keys())[0]\n",
        "    image = Image.open(image_filename)\n",
        "\n",
        "    # Run Inference\n",
        "    prompt = \"<image>\\\\nFree OCR.\"\n",
        "\n",
        "    res = model.infer(\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        image_file=image_filename,\n",
        "        output_path=\"./output\",\n",
        "        base_size=1024,\n",
        "        image_size=640,\n",
        "        crop_mode=True,\n",
        "        save_results=True,\n",
        "        test_compress=False\n",
        "    )\n",
        "\n",
        "    print(\"\\\\nOCR Result:\")\n",
        "    print(res)\n",
        "else:\n",
        "    print(\"No file uploaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "id": "s_3JmnsLvzVn",
        "outputId": "55350b54-251d-4edc-95e4-218628716a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "請上傳發票圖片:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-618645db-f9a6-43e6-9169-4d23ca155694\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-618645db-f9a6-43e6-9169-4d23ca155694\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt=prompt, image_file=image_file,\n",
        "    output_path = output_path,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    save_results = True,\n",
        "    test_compress = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd30-Yg3fEeK"
      },
      "source": [
        "With only 60 steps, we dramatically improved the transcription quality. The Character Error Rate (CER) on this single sample dropped from 23% to 6%, a 74% relative reduction!\n",
        "\n",
        "| Type | OCR |\n",
        "| :--- | :--- |\n",
        "| **Baseline (Pre-Finetune)** | `انضباطم نندم حقيقتن باورم نميتند` |\n",
        "| **Finetuned (60 steps)** | `انضباطم نشدم حقیقتن باورم نمیشد` |\n",
        "| **Ground Truth** | `انضباطمم شدم حقیقتن باورم نمیشد` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "        auto_model = AutoModel,\n",
        "        trust_remote_code=True,\n",
        "        unsloth_force_compile=True,\n",
        "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        "    )\n",
        "    FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt=prompt, image_file=image_file,\n",
        "    output_path = output_path,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    save_results = True,\n",
        "    test_compress = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmf5JAgHut6i"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dcf4425815be4f1e828e1613fac192c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fb70cc6203c4f0ba7b3eb8b5108ede5",
              "IPY_MODEL_211fc78c7964460d8da91cdc05a733f2",
              "IPY_MODEL_1891a0e06f204824b076a6a59c0a7417"
            ],
            "layout": "IPY_MODEL_44853c7f0a9f49a3bcfd37c812ced9bb"
          }
        },
        "1fb70cc6203c4f0ba7b3eb8b5108ede5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7b6271ce623488ab47bf523152cc550",
            "placeholder": "​",
            "style": "IPY_MODEL_8fc37e4b3c13469491264c50d5ec99ab",
            "value": "Fetching 21 files: 100%"
          }
        },
        "211fc78c7964460d8da91cdc05a733f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0150cd6a2914e1da57832da57ff7674",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42c3c2ca20504432995e86ea6ce95c19",
            "value": 21
          }
        },
        "1891a0e06f204824b076a6a59c0a7417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c77831542aa46519cd3aaa7ebd48ae4",
            "placeholder": "​",
            "style": "IPY_MODEL_1edb8cd1c45a48e9baca24beb47e4c16",
            "value": " 21/21 [00:00&lt;00:00, 663.93it/s]"
          }
        },
        "44853c7f0a9f49a3bcfd37c812ced9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b6271ce623488ab47bf523152cc550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fc37e4b3c13469491264c50d5ec99ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0150cd6a2914e1da57832da57ff7674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c3c2ca20504432995e86ea6ce95c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c77831542aa46519cd3aaa7ebd48ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1edb8cd1c45a48e9baca24beb47e4c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}